import 'dart:typed_data';
import 'dart:math' as math;
import 'dart:collection';
import 'package:flutter/foundation.dart';
import '../services/dual_engine_service.dart';
import '../services/native_audio_service.dart';

/// Advanced Real-time Pitch Tracking Architecture
/// ëª©í‘œ: ë§ˆì´í¬â†’UI ì§€ì—° < 120ms, ì •í™•ë„ > 95%
class AdvancedPitchTracker {
  // 1. ìœˆë„/í™‰ ì¬ì¡°ì • ì„¤ì •
  static const int kWindowSize = 2048; // ì•½ 42ms @ 48kHz
  static const int kHopSize = 512;     // ì•½ 10ms @ 48kHz
  static const double kTargetLatency = 0.120; // 120ms ëª©í‘œ
  
  // 2. VAD (Voice Activity Detection) ì„¤ì •
  static const double kVadEnergyThreshold = 0.01;
  static const double kVadZcrThreshold = 0.1;
  static const int kVadHangoverFrames = 5;
  
  // 3. ìŠ¤ë¬´ë”© ì„¤ì •
  static const int kMedianFilterSize = 5;
  static const double kViterbiTransitionCost = 10.0;
  
  // 4. ë§ ë²„í¼ ì„¤ì •
  static const int kRingBufferSeconds = 5;
  static const int kRingBufferSize = 48000 * kRingBufferSeconds;
  
  // ì„œë¹„ìŠ¤
  final DualEngineService _dualEngine = DualEngineService();
  final NativeAudioService _audioService = NativeAudioService.instance;
  
  // ë§ ë²„í¼
  final Queue<double> _ringBuffer = Queue<double>();
  int _writePosition = 0;
  int _readPosition = 0;
  
  // VAD ìƒíƒœ
  bool _isVoiceActive = false;
  int _hangoverCounter = 0;
  double _noiseFloor = 0.0;
  
  // ìŠ¤ë¬´ë”© ë²„í¼
  final List<double> _medianBuffer = [];
  final List<PitchFrame> _viterbiPath = [];
  
  // ì„±ëŠ¥ ë©”íŠ¸ë¦­
  int _totalFrames = 0;
  int _processedFrames = 0;
  double _averageLatency = 0.0;
  
  // ì‚¬ìš©ì í”„ë¡œí•„
  UserVoiceProfile? _userProfile;
  
  /// ì´ˆê¸°í™”
  Future<void> initialize() async {
    await _audioService.initialize();
    await _dualEngine.initialize();
    
    // ë…¸ì´ì¦ˆ í”Œë¡œì–´ ìº˜ë¦¬ë¸Œë ˆì´ì…˜
    await _calibrateNoiseFloor();
    
    print('ğŸš€ Advanced Pitch Tracker ì´ˆê¸°í™” ì™„ë£Œ');
    print('ğŸ“Š Window: ${kWindowSize}ìƒ˜í”Œ (${(kWindowSize/48000*1000).toStringAsFixed(1)}ms)');
    print('ğŸ“Š Hop: ${kHopSize}ìƒ˜í”Œ (${(kHopSize/48000*1000).toStringAsFixed(1)}ms)');
    print('ğŸ“Š ëª©í‘œ ì§€ì—°: ${(kTargetLatency*1000).toStringAsFixed(0)}ms');
  }
  
  /// ë…¸ì´ì¦ˆ í”Œë¡œì–´ ìº˜ë¦¬ë¸Œë ˆì´ì…˜
  Future<void> _calibrateNoiseFloor() async {
    print('ğŸ¤ ë…¸ì´ì¦ˆ í”Œë¡œì–´ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì¤‘...');
    
    final samples = <double>[];
    for (int i = 0; i < 10; i++) {
      final buffer = await _audioService.getRealtimeAudioBuffer();
      if (buffer != null && buffer.isNotEmpty) {
        final energy = _calculateEnergy(buffer);
        samples.add(energy);
      }
      await Future.delayed(const Duration(milliseconds: 100));
    }
    
    if (samples.isNotEmpty) {
      samples.sort();
      _noiseFloor = samples[samples.length ~/ 2] * 1.5; // ì¤‘ê°„ê°’ì˜ 1.5ë°°
      print('âœ… ë…¸ì´ì¦ˆ í”Œë¡œì–´: ${_noiseFloor.toStringAsFixed(4)}');
    }
  }
  
  /// ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘
  Stream<PitchResult> startTracking() async* {
    print('ğŸ™ï¸ ì‹¤ì‹œê°„ í”¼ì¹˜ íŠ¸ë˜í‚¹ ì‹œì‘');
    
    await _audioService.startRecording();
    final startTime = DateTime.now();
    
    while (true) {
      final frameStartTime = DateTime.now();
      
      // 1. ì˜¤ë””ì˜¤ ë²„í¼ íšë“
      final audioBuffer = await _audioService.getRealtimeAudioBuffer();
      if (audioBuffer == null || audioBuffer.isEmpty) {
        await Future.delayed(const Duration(milliseconds: 10));
        continue;
      }
      
      // 2. ë§ ë²„í¼ì— ì €ì¥
      _addToRingBuffer(audioBuffer);
      
      // 3. ì²˜ë¦¬í•  í”„ë ˆì„ ì¶”ì¶œ
      final frame = _extractFrame(kWindowSize);
      if (frame == null) continue;
      
      _totalFrames++;
      
      // 4. VAD ì²´í¬
      final vadResult = _performVAD(frame);
      if (!vadResult.isActive) {
        // ë¬´ì„± êµ¬ê°„ ìŠ¤í‚µ (ì—°ì‚°ëŸ‰ ì ˆê°)
        yield PitchResult(
          frequency: 0,
          confidence: 0,
          timestamp: DateTime.now(),
          isVoiced: false,
          latency: DateTime.now().difference(frameStartTime).inMilliseconds / 1000.0,
        );
        continue;
      }
      
      _processedFrames++;
      
      // 5. í”¼ì¹˜ ë¶„ì„ (CREPE/SPICE ë˜ëŠ” ê²½ëŸ‰ ëª¨ë¸)
      final pitchData = await _analyzePitch(frame);
      
      // 6. ìŠ¤ë¬´ë”© ì ìš©
      final smoothedPitch = _applySmoothing(pitchData);
      
      // 7. ì‚¬ìš©ì í”„ë¡œí•„ ì ìš©
      final adjustedPitch = _applyUserProfile(smoothedPitch);
      
      // 8. ë ˆì´í„´ì‹œ ê³„ì‚°
      final latency = DateTime.now().difference(frameStartTime).inMilliseconds / 1000.0;
      _averageLatency = (_averageLatency * 0.9) + (latency * 0.1);
      
      // 9. ê²°ê³¼ ë°˜í™˜
      yield PitchResult(
        frequency: adjustedPitch.frequency,
        confidence: adjustedPitch.confidence,
        timestamp: DateTime.now(),
        isVoiced: true,
        latency: latency,
        note: _frequencyToNote(adjustedPitch.frequency),
        cents: _calculateCents(adjustedPitch.frequency),
      );
      
      // 10. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
      if (_totalFrames % 100 == 0) {
        final reduction = (1 - _processedFrames / _totalFrames) * 100;
        print('ğŸ“Š VAD ì—°ì‚° ì ˆê°: ${reduction.toStringAsFixed(1)}%');
        print('ğŸ“Š í‰ê·  ë ˆì´í„´ì‹œ: ${(_averageLatency*1000).toStringAsFixed(1)}ms');
      }
    }
  }
  
  /// ë§ ë²„í¼ì— ë°ì´í„° ì¶”ê°€
  void _addToRingBuffer(List<double> samples) {
    for (final sample in samples) {
      if (_ringBuffer.length >= kRingBufferSize) {
        _ringBuffer.removeFirst();
      }
      _ringBuffer.add(sample);
    }
  }
  
  /// í”„ë ˆì„ ì¶”ì¶œ
  List<double>? _extractFrame(int size) {
    if (_ringBuffer.length < size) return null;
    
    final frame = <double>[];
    final bufferList = _ringBuffer.toList();
    final startIdx = bufferList.length - size;
    
    for (int i = startIdx; i < bufferList.length; i++) {
      frame.add(bufferList[i]);
    }
    
    return frame;
  }
  
  /// VAD (Voice Activity Detection)
  VADResult _performVAD(List<double> frame) {
    // ì—ë„ˆì§€ ê³„ì‚°
    final energy = _calculateEnergy(frame);
    
    // Zero Crossing Rate ê³„ì‚°
    final zcr = _calculateZCR(frame);
    
    // ìŒì„± í™œë™ íŒë‹¨
    final isActive = energy > _noiseFloor && energy > kVadEnergyThreshold;
    
    if (isActive) {
      _isVoiceActive = true;
      _hangoverCounter = kVadHangoverFrames;
    } else if (_hangoverCounter > 0) {
      _hangoverCounter--;
      _isVoiceActive = true;
    } else {
      _isVoiceActive = false;
    }
    
    return VADResult(
      isActive: _isVoiceActive,
      energy: energy,
      zcr: zcr,
    );
  }
  
  /// ì—ë„ˆì§€ ê³„ì‚°
  double _calculateEnergy(List<double> frame) {
    double sum = 0;
    for (final sample in frame) {
      sum += sample * sample;
    }
    return math.sqrt(sum / frame.length);
  }
  
  /// Zero Crossing Rate ê³„ì‚°
  double _calculateZCR(List<double> frame) {
    int crossings = 0;
    for (int i = 1; i < frame.length; i++) {
      if ((frame[i] >= 0) != (frame[i-1] >= 0)) {
        crossings++;
      }
    }
    return crossings / frame.length;
  }
  
  /// í”¼ì¹˜ ë¶„ì„
  Future<PitchData> _analyzePitch(List<double> frame) async {
    try {
      // ë¹ ë¥¸ ê²½ë¡œ: ë¡œì»¬ autocorrelation (< 5ms)
      if (_averageLatency > kTargetLatency * 0.8) {
        return _fastAutocorrelation(frame);
      }
      
      // ì •í™•í•œ ê²½ë¡œ: CREPE/SPICE (< 50ms)
      final float32Frame = Float32List.fromList(frame);
      final result = await _dualEngine.analyzePitch(float32Frame);
      
      return PitchData(
        frequency: result.frequency,
        confidence: result.confidence,
        timestamp: DateTime.now(),
      );
      
    } catch (e) {
      // í´ë°±: ë¹ ë¥¸ ë¡œì»¬ ë¶„ì„
      return _fastAutocorrelation(frame);
    }
  }
  
  /// ë¹ ë¥¸ Autocorrelation í”¼ì¹˜ ê²€ì¶œ
  PitchData _fastAutocorrelation(List<double> frame) {
    const minPeriod = 48000 ~/ 1000; // 1000Hz
    const maxPeriod = 48000 ~/ 50;   // 50Hz
    
    double maxCorr = 0;
    int bestPeriod = 0;
    
    for (int period = minPeriod; period <= maxPeriod; period++) {
      double corr = 0;
      for (int i = 0; i < frame.length - period; i++) {
        corr += frame[i] * frame[i + period];
      }
      
      if (corr > maxCorr) {
        maxCorr = corr;
        bestPeriod = period;
      }
    }
    
    final frequency = bestPeriod > 0 ? 48000.0 / bestPeriod : 0;
    final confidence = maxCorr / frame.length;
    
    return PitchData(
      frequency: frequency,
      confidence: confidence.clamp(0, 1),
      timestamp: DateTime.now(),
    );
  }
  
  /// ìŠ¤ë¬´ë”© ì ìš© (Median + Viterbi)
  PitchData _applySmoothing(PitchData raw) {
    // Median í•„í„°
    _medianBuffer.add(raw.frequency);
    if (_medianBuffer.length > kMedianFilterSize) {
      _medianBuffer.removeAt(0);
    }
    
    final sorted = List<double>.from(_medianBuffer)..sort();
    final median = sorted[sorted.length ~/ 2];
    
    // Viterbi ê²½ë¡œ ìµœì í™” (ê°„ë‹¨í•œ ë²„ì „)
    if (_viterbiPath.isNotEmpty) {
      final lastPitch = _viterbiPath.last.frequency;
      final distance = (median - lastPitch).abs();
      
      // ì „ì´ ë¹„ìš©ì´ ë„ˆë¬´ í¬ë©´ ì´ì „ ê°’ ìœ ì§€
      if (distance > kViterbiTransitionCost) {
        return PitchData(
          frequency: lastPitch,
          confidence: raw.confidence * 0.8,
          timestamp: raw.timestamp,
        );
      }
    }
    
    final smoothed = PitchData(
      frequency: median,
      confidence: raw.confidence,
      timestamp: raw.timestamp,
    );
    
    _viterbiPath.add(PitchFrame(
      frequency: median,
      confidence: raw.confidence,
      timestamp: raw.timestamp,
    ));
    
    if (_viterbiPath.length > 10) {
      _viterbiPath.removeAt(0);
    }
    
    return smoothed;
  }
  
  /// ì‚¬ìš©ì í”„ë¡œí•„ ì ìš©
  PitchData _applyUserProfile(PitchData pitch) {
    if (_userProfile == null) return pitch;
    
    // ì‚¬ìš©ì ìŒì—­ëŒ€ì— ë§ì¶° ì¡°ì •
    var adjustedFreq = pitch.frequency;
    
    if (adjustedFreq < _userProfile!.minFrequency) {
      adjustedFreq *= 2; // ì˜¥íƒ€ë¸Œ ìœ„ë¡œ
    } else if (adjustedFreq > _userProfile!.maxFrequency) {
      adjustedFreq /= 2; // ì˜¥íƒ€ë¸Œ ì•„ë˜ë¡œ
    }
    
    return PitchData(
      frequency: adjustedFreq,
      confidence: pitch.confidence * _userProfile!.confidenceMultiplier,
      timestamp: pitch.timestamp,
    );
  }
  
  /// ì£¼íŒŒìˆ˜ë¥¼ ìŒí‘œë¡œ ë³€í™˜
  String _frequencyToNote(double frequency) {
    if (frequency < 20 || frequency > 20000) return '';
    
    const notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'];
    const a4 = 440.0;
    
    final halfSteps = 12 * math.log(frequency / a4) / math.log(2);
    final noteIndex = (halfSteps.round() + 9 + 48) % 12;
    final octave = ((halfSteps.round() + 9 + 48) ~/ 12) - 1;
    
    return '${notes[noteIndex]}$octave';
  }
  
  /// ì„¼íŠ¸ ê³„ì‚°
  double _calculateCents(double frequency) {
    if (frequency < 20 || frequency > 20000) return 0;
    
    const a4 = 440.0;
    final halfSteps = 12 * math.log(frequency / a4) / math.log(2);
    final nearestNote = halfSteps.round();
    final cents = (halfSteps - nearestNote) * 100;
    
    return cents;
  }
  
  /// ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±/ì—…ë°ì´íŠ¸
  Future<void> createUserProfile() async {
    print('ğŸ¤ ì‚¬ìš©ì ìŒì„± í”„ë¡œí•„ ìƒì„± ì¤‘...');
    
    // 10ì´ˆê°„ ìŒì„± ìˆ˜ì§‘
    final frequencies = <double>[];
    final stopwatch = Stopwatch()..start();
    
    await for (final result in startTracking()) {
      if (result.isVoiced && result.confidence > 0.7) {
        frequencies.add(result.frequency);
      }
      
      if (stopwatch.elapsedMilliseconds > 10000) break;
    }
    
    if (frequencies.isNotEmpty) {
      frequencies.sort();
      
      _userProfile = UserVoiceProfile(
        minFrequency: frequencies[frequencies.length ~/ 10],
        maxFrequency: frequencies[frequencies.length * 9 ~/ 10],
        averageFrequency: frequencies[frequencies.length ~/ 2],
        confidenceMultiplier: 1.0,
      );
      
      print('âœ… ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ì™„ë£Œ');
      print('ğŸ“Š ìŒì—­ëŒ€: ${_userProfile!.minFrequency.toStringAsFixed(1)}Hz - ${_userProfile!.maxFrequency.toStringAsFixed(1)}Hz');
    }
  }
  
  /// ì •ë¦¬
  void dispose() {
    _audioService.stopRecording();
    _ringBuffer.clear();
    _medianBuffer.clear();
    _viterbiPath.clear();
  }
}

/// í”¼ì¹˜ ê²°ê³¼
class PitchResult {
  final double frequency;
  final double confidence;
  final DateTime timestamp;
  final bool isVoiced;
  final double latency;
  final String? note;
  final double? cents;
  
  const PitchResult({
    required this.frequency,
    required this.confidence,
    required this.timestamp,
    required this.isVoiced,
    required this.latency,
    this.note,
    this.cents,
  });
}

/// VAD ê²°ê³¼
class VADResult {
  final bool isActive;
  final double energy;
  final double zcr;
  
  const VADResult({
    required this.isActive,
    required this.energy,
    required this.zcr,
  });
}

/// í”¼ì¹˜ ë°ì´í„°
class PitchData {
  final double frequency;
  final double confidence;
  final DateTime timestamp;
  
  const PitchData({
    required this.frequency,
    required this.confidence,
    required this.timestamp,
  });
}

/// í”¼ì¹˜ í”„ë ˆì„ (Viterbiìš©)
class PitchFrame {
  final double frequency;
  final double confidence;
  final DateTime timestamp;
  
  const PitchFrame({
    required this.frequency,
    required this.confidence,
    required this.timestamp,
  });
}

/// ì‚¬ìš©ì ìŒì„± í”„ë¡œí•„
class UserVoiceProfile {
  final double minFrequency;
  final double maxFrequency;
  final double averageFrequency;
  final double confidenceMultiplier;
  
  const UserVoiceProfile({
    required this.minFrequency,
    required this.maxFrequency,
    required this.averageFrequency,
    required this.confidenceMultiplier,
  });
}