#!/usr/bin/env python3
"""
CREPE Server - ì‹¤ì œ ì‘ë™í•˜ëŠ” í”¼ì¹˜ ë¶„ì„ ì„œë²„
Googleì˜ CREPE ëª¨ë¸ì„ ì‚¬ìš©í•œ ê³ í’ˆì§ˆ ë‹¨ì¼ í”¼ì¹˜ ì¶”ì 
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # TensorFlow ê²½ê³  ìˆ¨ê¸°ê¸°

from flask import Flask, request, jsonify
from flask_cors import CORS
import numpy as np
import base64
import crepe
import resampy
import io
import wave
import struct

app = Flask(__name__)
CORS(app)

# CREPE ëª¨ë¸ ì‚¬ì „ ë¡œë“œ (ì²« ìš”ì²­ ì§€ì—° ë°©ì§€)
print("CREPE ëª¨ë¸ ë¡œë”© ì¤‘...")
_ = crepe.predict(np.zeros(16000), 16000, viterbi=False, verbose=0)
print("CREPE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

@app.route('/health', methods=['GET'])
def health():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        'status': 'healthy',
        'model': 'CREPE',
        'version': '1.0.0',
        'description': 'Convolutional Representation for Pitch Estimation'
    })

@app.route('/analyze', methods=['POST'])
def analyze():
    """
    ì˜¤ë””ì˜¤ ë°ì´í„°ì˜ í”¼ì¹˜ ë¶„ì„
    
    Request:
    {
        "audio_base64": "base64 encoded audio",
        "sample_rate": 44100
    }
    
    Response:
    {
        "pitches": [261.63, 293.66, ...],
        "confidences": [0.95, 0.87, ...],
        "timestamps": [0.0, 0.01, ...],
        "statistics": {...}
    }
    """
    try:
        data = request.json
        
        # Base64 ë””ì½”ë”©
        audio_base64 = data.get('audio_base64', '')
        sample_rate = data.get('sample_rate', 44100)
        
        if not audio_base64:
            return jsonify({'error': 'No audio data provided'}), 400
        
        # Base64 â†’ ë°”ì´íŠ¸ ë°°ì—´
        audio_bytes = base64.b64decode(audio_base64)
        
        # WAV íŒŒì¼ì¸ ê²½ìš° ì²˜ë¦¬
        if audio_bytes[:4] == b'RIFF':
            # WAV í—¤ë” íŒŒì‹±
            with io.BytesIO(audio_bytes) as wav_io:
                with wave.open(wav_io, 'rb') as wav_file:
                    n_channels = wav_file.getnchannels()
                    sample_width = wav_file.getsampwidth()
                    framerate = wav_file.getframerate()
                    n_frames = wav_file.getnframes()
                    
                    # ì˜¤ë””ì˜¤ ë°ì´í„° ì½ê¸°
                    frames = wav_file.readframes(n_frames)
                    
                    # 16ë¹„íŠ¸ ì •ìˆ˜ë¥¼ floatë¡œ ë³€í™˜
                    if sample_width == 2:
                        audio_int16 = struct.unpack(f'{n_frames * n_channels}h', frames)
                        audio = np.array(audio_int16, dtype=np.float32) / 32768.0
                    else:
                        audio = np.frombuffer(frames, dtype=np.float32)
                    
                    # ìŠ¤í…Œë ˆì˜¤ë¥¼ ëª¨ë…¸ë¡œ ë³€í™˜
                    if n_channels == 2:
                        audio = audio.reshape(-1, 2).mean(axis=1)
                    
                    sample_rate = framerate
        else:
            # Raw float32 ë°ì´í„°ë¡œ ê°€ì •
            audio = np.frombuffer(audio_bytes, dtype=np.float32)
        
        # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
        if len(audio) == 0:
            return jsonify({'error': 'Empty audio data'}), 400
        
        # CREPEëŠ” 16kHzë¥¼ ì„ í˜¸í•˜ë¯€ë¡œ ë¦¬ìƒ˜í”Œë§
        if sample_rate != 16000:
            audio = resampy.resample(audio, sample_rate, 16000)
            sample_rate = 16000
        
        # ì˜¤ë””ì˜¤ ì •ê·œí™” (-1 ~ 1)
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val
        
        # CREPE ë¶„ì„ ì‹¤í–‰
        # step_size: 10ms (ë” ì„¸ë°€í•œ ë¶„ì„)
        # model_capacity: full (ìµœê³  í’ˆì§ˆ)
        # viterbi: True (ì‹œê°„ì  ì—°ì†ì„± ê°œì„ )
        time, frequency, confidence, activation = crepe.predict(
            audio, 
            sample_rate,
            step_size=10,
            model_capacity='full',
            viterbi=True,
            verbose=0
        )
        
        # NaN ê°’ ì²˜ë¦¬
        valid_indices = ~np.isnan(frequency)
        time = time[valid_indices]
        frequency = frequency[valid_indices]
        confidence = confidence[valid_indices]
        
        # ë‚®ì€ ì‹ ë¢°ë„ í•„í„°ë§ (0.5 ì´í•˜)
        high_conf_indices = confidence > 0.5
        time = time[high_conf_indices]
        frequency = frequency[high_conf_indices]
        confidence = confidence[high_conf_indices]
        
        # í†µê³„ ê³„ì‚°
        statistics = {}
        if len(frequency) > 0:
            statistics = {
                'mean_pitch': float(np.mean(frequency)),
                'std_pitch': float(np.std(frequency)),
                'min_pitch': float(np.min(frequency)),
                'max_pitch': float(np.max(frequency)),
                'mean_confidence': float(np.mean(confidence)),
                'pitch_range': float(np.max(frequency) - np.min(frequency)),
                'num_frames': len(frequency)
            }
        
        # ê²°ê³¼ ë°˜í™˜
        return jsonify({
            'pitches': frequency.tolist(),
            'confidences': confidence.tolist(),
            'timestamps': time.tolist(),
            'statistics': statistics,
            'sample_rate': 16000,
            'model': 'CREPE-full',
            'step_size_ms': 10
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/analyze_chunked', methods=['POST'])
def analyze_chunked():
    """
    ê¸´ ì˜¤ë””ì˜¤ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ì„
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²˜ë¦¬
    """
    try:
        data = request.json
        audio_base64 = data.get('audio_base64', '')
        sample_rate = data.get('sample_rate', 44100)
        chunk_duration = data.get('chunk_duration', 5)  # 5ì´ˆ ë‹¨ìœ„
        
        if not audio_base64:
            return jsonify({'error': 'No audio data provided'}), 400
        
        # Base64 ë””ì½”ë”©
        audio_bytes = base64.b64decode(audio_base64)
        
        # ì˜¤ë””ì˜¤ ë¡œë“œ (WAV ì²˜ë¦¬ ë¡œì§ ë™ì¼)
        if audio_bytes[:4] == b'RIFF':
            with io.BytesIO(audio_bytes) as wav_io:
                with wave.open(wav_io, 'rb') as wav_file:
                    n_channels = wav_file.getnchannels()
                    sample_width = wav_file.getsampwidth()
                    framerate = wav_file.getframerate()
                    n_frames = wav_file.getnframes()
                    frames = wav_file.readframes(n_frames)
                    
                    if sample_width == 2:
                        audio_int16 = struct.unpack(f'{n_frames * n_channels}h', frames)
                        audio = np.array(audio_int16, dtype=np.float32) / 32768.0
                    else:
                        audio = np.frombuffer(frames, dtype=np.float32)
                    
                    if n_channels == 2:
                        audio = audio.reshape(-1, 2).mean(axis=1)
                    
                    sample_rate = framerate
        else:
            audio = np.frombuffer(audio_bytes, dtype=np.float32)
        
        # ë¦¬ìƒ˜í”Œë§
        if sample_rate != 16000:
            audio = resampy.resample(audio, sample_rate, 16000)
            sample_rate = 16000
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ì„
        chunk_size = int(chunk_duration * sample_rate)
        all_pitches = []
        all_confidences = []
        all_timestamps = []
        
        for i in range(0, len(audio), chunk_size):
            chunk = audio[i:i+chunk_size]
            
            if len(chunk) < sample_rate:  # 1ì´ˆ ë¯¸ë§Œì€ ê±´ë„ˆë›°ê¸°
                continue
            
            # ì²­í¬ ì •ê·œí™”
            max_val = np.max(np.abs(chunk))
            if max_val > 0:
                chunk = chunk / max_val
            
            # CREPE ë¶„ì„
            time, frequency, confidence, _ = crepe.predict(
                chunk,
                sample_rate,
                step_size=10,
                model_capacity='full',
                viterbi=True,
                verbose=0
            )
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¡°ì • (ì²­í¬ ì˜¤í”„ì…‹ ì¶”ê°€)
            time_offset = i / sample_rate
            time = time + time_offset
            
            # ìœ íš¨í•œ ê°’ë§Œ ì¶”ê°€
            valid = ~np.isnan(frequency) & (confidence > 0.5)
            all_timestamps.extend(time[valid].tolist())
            all_pitches.extend(frequency[valid].tolist())
            all_confidences.extend(confidence[valid].tolist())
        
        # í†µê³„
        statistics = {}
        if all_pitches:
            statistics = {
                'mean_pitch': float(np.mean(all_pitches)),
                'std_pitch': float(np.std(all_pitches)),
                'min_pitch': float(np.min(all_pitches)),
                'max_pitch': float(np.max(all_pitches)),
                'mean_confidence': float(np.mean(all_confidences)),
                'total_duration': float(len(audio) / sample_rate),
                'num_chunks': int(np.ceil(len(audio) / chunk_size))
            }
        
        return jsonify({
            'pitches': all_pitches,
            'confidences': all_confidences,
            'timestamps': all_timestamps,
            'statistics': statistics,
            'chunk_duration': chunk_duration,
            'model': 'CREPE-full-chunked'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    print("=" * 50)
    print("ğŸµ CREPE Server - ì‹¤ì œ í”¼ì¹˜ ë¶„ì„ ì„œë²„")
    print("=" * 50)
    print("í¬íŠ¸: 5002")
    print("ëª¨ë¸: CREPE (Convolutional Neural Network)")
    print("ì •í™•ë„: ìµœê³  í’ˆì§ˆ (full capacity)")
    print("=" * 50)
    print("ì—”ë“œí¬ì¸íŠ¸:")
    print("  GET  /health          - ì„œë²„ ìƒíƒœ")
    print("  POST /analyze         - í”¼ì¹˜ ë¶„ì„")
    print("  POST /analyze_chunked - ì²­í¬ ë‹¨ìœ„ ë¶„ì„")
    print("=" * 50)
    
    app.run(host='0.0.0.0', port=5002, debug=False)