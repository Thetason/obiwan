#!/usr/bin/env python3
"""
SPICE Server - ì‹¤ì œ ì‘ë™í•˜ëŠ” í”¼ì¹˜ ë¶„ì„ ì„œë²„
Googleì˜ SPICE ëª¨ë¸ì„ ì‚¬ìš©í•œ Self-supervised í”¼ì¹˜ ì¶”ì •
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

from flask import Flask, request, jsonify
from flask_cors import CORS
import numpy as np
import base64
import tensorflow as tf
import tensorflow_hub as hub
import resampy
import io
import wave
import struct

app = Flask(__name__)
CORS(app)

# SPICE ëª¨ë¸ ë¡œë“œ
print("SPICE ëª¨ë¸ ë¡œë”© ì¤‘...")
model = hub.load("https://tfhub.dev/google/spice/2")
print("SPICE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

# ìŒê³„ ì •ì˜ (A0 ~ C8)
A4 = 440
C0 = A4 * np.power(2, -4.75)
note_names = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"]

def hz2offset(freq):
    """ì£¼íŒŒìˆ˜ë¥¼ MIDI í”¼ì¹˜ ì˜¤í”„ì…‹ìœ¼ë¡œ ë³€í™˜"""
    if freq <= 0:
        return 0
    return 12 * np.log2(freq / C0)

def quantize_predictions(freqs, confs):
    """ì˜ˆì¸¡ê°’ì„ ê°€ì¥ ê°€ê¹Œìš´ ìŒê³„ë¡œ ì–‘ìí™”"""
    quantized_freqs = []
    quantized_notes = []
    
    for freq, conf in zip(freqs, confs):
        if conf < 0.5 or freq <= 0:
            quantized_freqs.append(0)
            quantized_notes.append("Rest")
        else:
            offset = hz2offset(freq)
            n = int(np.round(offset))
            cents = (offset - n) * 100
            
            note_idx = n % 12
            octave = n // 12
            
            note_name = note_names[note_idx] + str(octave)
            
            # ì •í™•í•œ ìŒê³„ ì£¼íŒŒìˆ˜ ê³„ì‚°
            quantized_freq = C0 * np.power(2, n / 12)
            quantized_freqs.append(quantized_freq)
            quantized_notes.append(note_name)
    
    return quantized_freqs, quantized_notes

@app.route('/health', methods=['GET'])
def health():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        'status': 'healthy',
        'model': 'SPICE',
        'version': '2.0',
        'description': 'Self-supervised Pitch Estimation'
    })

@app.route('/analyze', methods=['POST'])
def analyze():
    """
    ì˜¤ë””ì˜¤ ë°ì´í„°ì˜ í”¼ì¹˜ ë¶„ì„
    
    Request:
    {
        "audio_base64": "base64 encoded audio",
        "sample_rate": 44100
    }
    
    Response:
    {
        "pitches": [261.63, 293.66, ...],
        "confidences": [0.95, 0.87, ...],
        "notes": ["C4", "D4", ...],
        "timestamps": [0.0, 0.032, ...],
        "statistics": {...}
    }
    """
    try:
        data = request.json
        
        # Base64 ë””ì½”ë”©
        audio_base64 = data.get('audio_base64', '')
        sample_rate = data.get('sample_rate', 44100)
        
        if not audio_base64:
            return jsonify({'error': 'No audio data provided'}), 400
        
        # Base64 â†’ ë°”ì´íŠ¸ ë°°ì—´
        audio_bytes = base64.b64decode(audio_base64)
        
        # WAV íŒŒì¼ì¸ ê²½ìš° ì²˜ë¦¬
        if audio_bytes[:4] == b'RIFF':
            with io.BytesIO(audio_bytes) as wav_io:
                with wave.open(wav_io, 'rb') as wav_file:
                    n_channels = wav_file.getnchannels()
                    sample_width = wav_file.getsampwidth()
                    framerate = wav_file.getframerate()
                    n_frames = wav_file.getnframes()
                    frames = wav_file.readframes(n_frames)
                    
                    if sample_width == 2:
                        audio_int16 = struct.unpack(f'{n_frames * n_channels}h', frames)
                        audio = np.array(audio_int16, dtype=np.float32) / 32768.0
                    else:
                        audio = np.frombuffer(frames, dtype=np.float32)
                    
                    if n_channels == 2:
                        audio = audio.reshape(-1, 2).mean(axis=1)
                    
                    sample_rate = framerate
        else:
            # Raw float32 ë°ì´í„°
            audio = np.frombuffer(audio_bytes, dtype=np.float32)
        
        # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
        if len(audio) == 0:
            return jsonify({'error': 'Empty audio data'}), 400
        
        # SPICEëŠ” 16kHzë¥¼ í•„ìš”ë¡œ í•¨
        if sample_rate != 16000:
            audio = resampy.resample(audio, sample_rate, 16000)
            sample_rate = 16000
        
        # ì •ê·œí™”
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val
        
        # TensorFlow í…ì„œë¡œ ë³€í™˜
        audio_tensor = tf.constant(audio, dtype=tf.float32)
        
        # SPICE ëª¨ë¸ ì‹¤í–‰
        model_output = model.signatures["serving_default"](
            tf.expand_dims(audio_tensor, 0)
        )
        
        # í”¼ì¹˜ì™€ ì‹ ë¢°ë„ ì¶”ì¶œ
        pitch_outputs = model_output["pitch"]
        uncertainty_outputs = model_output["uncertainty"]
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ë¶ˆí™•ì‹¤ì„±ì˜ ì—­)
        confidences = 1.0 - uncertainty_outputs[0]
        
        # Hzë¡œ ë³€í™˜ (SPICEëŠ” MIDI í”¼ì¹˜ë¥¼ ì¶œë ¥)
        frequencies = []
        for pitch in pitch_outputs[0]:
            if pitch > 0:
                freq = C0 * np.power(2, pitch / 12)
            else:
                freq = 0
            frequencies.append(freq)
        
        frequencies = np.array(frequencies)
        confidences = np.array(confidences)
        
        # ìœ íš¨í•œ í”¼ì¹˜ë§Œ í•„í„°ë§ (ì‹ ë¢°ë„ > 0.5)
        valid_indices = confidences > 0.5
        frequencies = frequencies[valid_indices]
        confidences = confidences[valid_indices]
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (32ms ê°„ê²©)
        time_step = 0.032  # SPICEì˜ ê¸°ë³¸ í”„ë ˆì„ ê°„ê²©
        timestamps = np.arange(len(frequencies)) * time_step
        
        # ìŒê³„ë¡œ ì–‘ìí™”
        quantized_freqs, note_names = quantize_predictions(frequencies, confidences)
        
        # í†µê³„ ê³„ì‚°
        statistics = {}
        if len(frequencies) > 0:
            valid_freqs = frequencies[frequencies > 0]
            if len(valid_freqs) > 0:
                statistics = {
                    'mean_pitch': float(np.mean(valid_freqs)),
                    'std_pitch': float(np.std(valid_freqs)),
                    'min_pitch': float(np.min(valid_freqs)),
                    'max_pitch': float(np.max(valid_freqs)),
                    'mean_confidence': float(np.mean(confidences)),
                    'pitch_range': float(np.max(valid_freqs) - np.min(valid_freqs)),
                    'num_frames': len(frequencies),
                    'total_duration': float(len(audio) / sample_rate)
                }
        
        # ê²°ê³¼ ë°˜í™˜
        return jsonify({
            'pitches': frequencies.tolist(),
            'confidences': confidences.tolist(),
            'quantized_pitches': quantized_freqs,
            'notes': note_names,
            'timestamps': timestamps.tolist(),
            'statistics': statistics,
            'sample_rate': 16000,
            'model': 'SPICE-v2',
            'frame_step_ms': 32
        })
        
    except Exception as e:
        import traceback
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/analyze_polyphonic', methods=['POST'])
def analyze_polyphonic():
    """
    ë‹¤ì„± ìŒì•… ë¶„ì„ (ì‹¤í—˜ì )
    ì—¬ëŸ¬ ìŒì •ì„ ë™ì‹œì— ê°ì§€ ì‹œë„
    """
    try:
        data = request.json
        audio_base64 = data.get('audio_base64', '')
        sample_rate = data.get('sample_rate', 44100)
        
        if not audio_base64:
            return jsonify({'error': 'No audio data provided'}), 400
        
        # ì˜¤ë””ì˜¤ ë””ì½”ë”© (ë™ì¼í•œ ë¡œì§)
        audio_bytes = base64.b64decode(audio_base64)
        
        if audio_bytes[:4] == b'RIFF':
            with io.BytesIO(audio_bytes) as wav_io:
                with wave.open(wav_io, 'rb') as wav_file:
                    n_channels = wav_file.getnchannels()
                    sample_width = wav_file.getsampwidth()
                    framerate = wav_file.getframerate()
                    n_frames = wav_file.getnframes()
                    frames = wav_file.readframes(n_frames)
                    
                    if sample_width == 2:
                        audio_int16 = struct.unpack(f'{n_frames * n_channels}h', frames)
                        audio = np.array(audio_int16, dtype=np.float32) / 32768.0
                    else:
                        audio = np.frombuffer(frames, dtype=np.float32)
                    
                    if n_channels == 2:
                        # ìŠ¤í…Œë ˆì˜¤ ì±„ë„ ë¶„ë¦¬ (ë‹¤ì„± ë¶„ì„ìš©)
                        audio_stereo = audio.reshape(-1, 2)
                        audio_left = audio_stereo[:, 0]
                        audio_right = audio_stereo[:, 1]
                    else:
                        audio_left = audio
                        audio_right = audio
                    
                    sample_rate = framerate
        else:
            audio = np.frombuffer(audio_bytes, dtype=np.float32)
            audio_left = audio
            audio_right = audio
        
        # ê° ì±„ë„ ë¦¬ìƒ˜í”Œë§
        if sample_rate != 16000:
            audio_left = resampy.resample(audio_left, sample_rate, 16000)
            audio_right = resampy.resample(audio_right, sample_rate, 16000)
            sample_rate = 16000
        
        # ê° ì±„ë„ ë¶„ì„
        results_left = analyze_channel(audio_left)
        results_right = analyze_channel(audio_right)
        
        # ê²°ê³¼ ë³‘í•©
        all_pitches = []
        all_notes = []
        
        for i in range(min(len(results_left['pitches']), len(results_right['pitches']))):
            pitch_left = results_left['pitches'][i]
            pitch_right = results_right['pitches'][i]
            
            pitches_frame = []
            notes_frame = []
            
            if pitch_left > 0:
                pitches_frame.append(pitch_left)
                notes_frame.append(results_left['notes'][i])
            
            if pitch_right > 0 and abs(pitch_right - pitch_left) > 20:  # 20Hz ì´ìƒ ì°¨ì´
                pitches_frame.append(pitch_right)
                notes_frame.append(results_right['notes'][i])
            
            all_pitches.append(pitches_frame)
            all_notes.append(notes_frame)
        
        return jsonify({
            'polyphonic_pitches': all_pitches,
            'polyphonic_notes': all_notes,
            'left_channel': results_left,
            'right_channel': results_right,
            'model': 'SPICE-v2-polyphonic'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def analyze_channel(audio):
    """ë‹¨ì¼ ì±„ë„ ë¶„ì„ í—¬í¼ í•¨ìˆ˜"""
    # ì •ê·œí™”
    max_val = np.max(np.abs(audio))
    if max_val > 0:
        audio = audio / max_val
    
    # SPICE ëª¨ë¸ ì‹¤í–‰
    audio_tensor = tf.constant(audio, dtype=tf.float32)
    model_output = model.signatures["serving_default"](
        tf.expand_dims(audio_tensor, 0)
    )
    
    pitch_outputs = model_output["pitch"]
    uncertainty_outputs = model_output["uncertainty"]
    confidences = 1.0 - uncertainty_outputs[0]
    
    # Hzë¡œ ë³€í™˜
    frequencies = []
    for pitch in pitch_outputs[0]:
        if pitch > 0:
            freq = C0 * np.power(2, pitch / 12)
        else:
            freq = 0
        frequencies.append(freq)
    
    # ìŒê³„ ì–‘ìí™”
    quantized_freqs, note_names = quantize_predictions(frequencies, confidences)
    
    return {
        'pitches': frequencies,
        'notes': note_names,
        'confidences': confidences.numpy().tolist()
    }

if __name__ == '__main__':
    print("=" * 50)
    print("ğŸµ SPICE Server - Self-supervised í”¼ì¹˜ ë¶„ì„")
    print("=" * 50)
    print("í¬íŠ¸: 5003")
    print("ëª¨ë¸: SPICE v2 (Google Research)")
    print("íŠ¹ì§•: ìê°€ í•™ìŠµ, ìŒê³„ ì–‘ìí™”")
    print("=" * 50)
    print("ì—”ë“œí¬ì¸íŠ¸:")
    print("  GET  /health              - ì„œë²„ ìƒíƒœ")
    print("  POST /analyze             - ë‹¨ì„± í”¼ì¹˜ ë¶„ì„")
    print("  POST /analyze_polyphonic  - ë‹¤ì„± ë¶„ì„ (ì‹¤í—˜ì )")
    print("=" * 50)
    
    app.run(host='0.0.0.0', port=5003, debug=False)