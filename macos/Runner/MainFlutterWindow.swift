import Cocoa
import FlutterMacOS
import AVFoundation
import CoreML

class RealTimeAudioRecorder: NSObject, AVAudioPlayerDelegate {
  private var audioEngine: AVAudioEngine?
  private var inputNode: AVAudioInputNode?
  private var channel: FlutterMethodChannel?
  private var isRecording = false
  private var recordedSamples: [Float] = []
  private var recordingStartTime: Date?
  private var audioPlayer: AVAudioPlayer?
  private var playbackTimer: Timer?
  private var tempFileURL: URL?  // ì„ì‹œ íŒŒì¼ ì¶”ì 
  private var isDisposed = false  // dispose ìƒíƒœ ì¶”ì 
  
  init(channel: FlutterMethodChannel) {
    self.channel = channel
    super.init()
    setupAudioEngine()
  }
  
  // Public method to get current audio level
  func getCurrentAudioLevel() -> Double {
    guard recordedSamples.count > 0 else { return 0.0 }
    
    // Calculate RMS from recent samples
    let recentCount = min(100, recordedSamples.count)
    let startIdx = recordedSamples.count - recentCount
    let recentSamples = Array(recordedSamples[startIdx..<recordedSamples.count])
    let rms = sqrt(recentSamples.map { $0 * $0 }.reduce(0, +) / Float(recentSamples.count))
    return Double(rms)
  }
  
  deinit {
    print("ğŸ§¹ [RealTime] RealTimeAudioRecorder deinit í˜¸ì¶œë¨")
    isDisposed = true
    
    // CRITICAL FIX: íƒ€ì´ë¨¸ë¥¼ ë¨¼ì € ì •ë¦¬ (ì½œë°± ë°©ì§€)
    playbackTimer?.invalidate()
    playbackTimer = nil
    
    // CRITICAL FIX: audioPlayer delegateë¥¼ nilë¡œ ì„¤ì • í›„ ì •ì§€
    if let player = audioPlayer {
      player.delegate = nil  // delegate í•´ì œë¡œ ì½œë°± ë°©ì§€
      player.stop()
      audioPlayer = nil
    }
    
    // ë…¹ìŒ ì¤‘ì´ë©´ ì¤‘ì§€
    if isRecording {
      inputNode?.removeTap(onBus: 0)
      audioEngine?.stop()
      isRecording = false
    }
    
    // CRITICAL FIX: ì„ì‹œ íŒŒì¼ ì•ˆì „ ì‚­ì œ
    if let tempURL = tempFileURL {
      DispatchQueue.global(qos: .background).async {
        try? FileManager.default.removeItem(at: tempURL)
      }
      tempFileURL = nil
    }
    
    // ì—”ì§„ ì •ë¦¬
    audioEngine = nil
    inputNode = nil
    
    // ìƒ˜í”Œ ë°ì´í„° ì •ë¦¬
    recordedSamples.removeAll()
    
    print("âœ… [RealTime] RealTimeAudioRecorder ì •ë¦¬ ì™„ë£Œ")
  }
  
  private func setupAudioEngine() {
    print("ğŸ”§ [RealTime] ì˜¤ë””ì˜¤ ì—”ì§„ ì…‹ì—… ì‹œì‘")
    
    // ê¸°ì¡´ ì—”ì§„ì´ ìˆìœ¼ë©´ ì™„ì „íˆ ì •ë¦¬
    if let engine = audioEngine {
      print("ğŸ”§ [RealTime] ê¸°ì¡´ ì—”ì§„ ì •ë¦¬ ì¤‘ (ì‹¤í–‰ìƒíƒœ: \(engine.isRunning))")
      if engine.isRunning {
        engine.stop()
      }
      // ê¸°ì¡´ íƒ­ë„ ì œê±°
      inputNode?.removeTap(onBus: 0)
      audioEngine = nil
      inputNode = nil
    }
    
    // ìƒˆ ì—”ì§„ ìƒì„±
    audioEngine = AVAudioEngine()
    inputNode = audioEngine?.inputNode
    
    // ì—”ì§„ê³¼ ì…ë ¥ ë…¸ë“œ ìœ íš¨ì„± ê²€ì‚¬
    guard let engine = audioEngine else {
      print("âŒ [RealTime] ì˜¤ë””ì˜¤ ì—”ì§„ ìƒì„± ì‹¤íŒ¨")
      return
    }
    
    guard let input = inputNode else {
      print("âŒ [RealTime] ì…ë ¥ ë…¸ë“œ ìƒì„± ì‹¤íŒ¨")
      return
    }
    
    // ì…ë ¥ ì¥ì¹˜ ì •ë³´ ìƒì„¸ ì¶œë ¥
    let inputFormat = input.outputFormat(forBus: 0)
    print("âœ… [RealTime] ì˜¤ë””ì˜¤ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    print("ğŸ›ï¸ [RealTime] ì—”ì§„ ìƒíƒœ: isRunning=\(engine.isRunning)")
    print("ğŸ¤ [RealTime] ì…ë ¥ ë…¸ë“œ í¬ë§·: \(inputFormat)")
    print("ğŸ“Š [RealTime] ìƒ˜í”Œë ˆì´íŠ¸: \(inputFormat.sampleRate)Hz")
    print("ğŸ“Š [RealTime] ì±„ë„ ìˆ˜: \(inputFormat.channelCount)")
    print("ğŸ“Š [RealTime] ì¸í„°ë¦¬ë¸Œ: \(inputFormat.isInterleaved)")
    
    // ì…ë ¥ ì¥ì¹˜ê°€ ì œëŒ€ë¡œ ì—°ê²°ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if inputFormat.channelCount == 0 {
      print("âš ï¸ [RealTime] ê²½ê³ : ì…ë ¥ ì±„ë„ì´ 0ê°œì…ë‹ˆë‹¤. ë§ˆì´í¬ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    }
    
    // CRITICAL: ì˜¤ë””ì˜¤ ì„¸ì…˜ êµ¬ì„± (macOSì—ì„œëŠ” ë¶ˆí•„ìš”í•˜ì§€ë§Œ í™•ì¸ìš©)
    print("ğŸ™ï¸ [RealTime] ì˜¤ë””ì˜¤ ì„¸ì…˜ êµ¬ì„± ì™„ë£Œ")
  }
  
  func startRecording(result: @escaping FlutterResult) {
    print("ğŸ™ï¸ [RealTime] ë…¹ìŒ ì‹œì‘ ìš”ì²­")
    
    // 1. ë§ˆì´í¬ ê¶Œí•œ í™•ì¸
    let status = AVCaptureDevice.authorizationStatus(for: .audio)
    print("ğŸ” [RealTime] ë§ˆì´í¬ ê¶Œí•œ ìƒíƒœ: \(status.rawValue)")
    
    guard status == .authorized else {
      print("âŒ [RealTime] ë§ˆì´í¬ ê¶Œí•œ ì—†ìŒ")
      result(FlutterError(code: "PERMISSION_DENIED", message: "Microphone permission required", details: nil))
      return
    }
    
    // ì—”ì§„ì´ ì—†ìœ¼ë©´ ë‹¤ì‹œ ì´ˆê¸°í™”
    if audioEngine == nil {
      print("ğŸ”§ [RealTime] ì˜¤ë””ì˜¤ ì—”ì§„ì´ nil - ì¬ì´ˆê¸°í™”")
      setupAudioEngine()
    }
    
    guard let engine = audioEngine, let input = inputNode else {
      print("âŒ [RealTime] ì˜¤ë””ì˜¤ ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨")
      result(FlutterError(code: "ENGINE_NOT_READY", message: "Audio engine not ready", details: nil))
      return
    }
    
    if isRecording {
      print("âš ï¸ [RealTime] ì´ë¯¸ ë…¹ìŒ ì¤‘")
      result(true)
      return
    }
    
    do {
      // 2. ìƒ˜í”Œ ë°°ì—´ ì´ˆê¸°í™”
      recordedSamples.removeAll()
      recordingStartTime = Date()
      
      // 3. ì…ë ¥ í¬ë§· í™•ì¸
      let inputFormat = input.outputFormat(forBus: 0)
      print("ğŸ›ï¸ [RealTime] ì…ë ¥ í¬ë§·: \(inputFormat.sampleRate)Hz, \(inputFormat.channelCount)ì±„ë„")
      
      // 4. ê¸°ì¡´ íƒ­ ì œê±° (ì•ˆì „ ì¡°ì¹˜)
      input.removeTap(onBus: 0)
      print("ğŸ”§ [RealTime] ê¸°ì¡´ íƒ­ ì œê±° ì™„ë£Œ")
      
      // 5. ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬ íƒ­ ì„¤ì¹˜ - CRITICAL FIX
      let bufferSize: AVAudioFrameCount = 1024  // ë” ì‘ì€ ë²„í¼ë¡œ ì‹œë„
      
      print("ğŸ”§ [RealTime] íƒ­ ì„¤ì¹˜ ì‹œë„ - ë²„í¼: \(bufferSize)")
      print("ğŸ”§ [RealTime] ì…ë ¥ í¬ë§·: \(inputFormat)")
      
      // CRITICAL FIX: ì…ë ¥ ë…¸ë“œì˜ ì‹¤ì œ í¬ë§· ì‚¬ìš© (ëª…ì‹œì  í¬ë§· ì§€ì •í•˜ì§€ ì•ŠìŒ)
      print("ğŸ”§ [RealTime] ì…ë ¥ ë…¸ë“œì˜ ì‹¤ì œ í¬ë§· ì‚¬ìš©")
      
      // ì…ë ¥ í¬ë§·ì´ ìœ íš¨í•œì§€ ë‹¤ì‹œ í™•ì¸
      if inputFormat.channelCount == 0 {
        print("âŒ [RealTime] ì…ë ¥ ì±„ë„ì´ 0ê°œ - ë§ˆì´í¬ ì—°ê²° í™•ì¸ í•„ìš”")
        result(FlutterError(code: "NO_INPUT", message: "No audio input channels available", details: nil))
        return
      }
      
      // CRITICAL FIX: macOSì—ì„œëŠ” ëª…ì‹œì  í¬ë§· ì§€ì •ì´ í•„ìš”
      // 48kHzë¡œ ëª…ì‹œì  í¬ë§· ì„¤ì •
      let recordingFormat = AVAudioFormat(standardFormatWithSampleRate: 48000, channels: 1)!
      print("ğŸ¤ [RealTime] ë…¹ìŒ í¬ë§· ì„¤ì •: 48kHz, 1ì±„ë„")
      
      // íƒ­ ì„¤ì¹˜ - ëª…ì‹œì  í¬ë§· ì‚¬ìš©
      input.installTap(onBus: 0, bufferSize: bufferSize, format: recordingFormat) { [weak self] (buffer, when) in
        // ì¦‰ì‹œ ë¡œê¹… - ì–´ë–¤ ìŠ¤ë ˆë“œì—ì„œë“  í˜¸ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
        print("ğŸ“¥ [RealTime] *** TAP CALLBACK RECEIVED *** - frameLength: \(buffer.frameLength)")
        
        // ê°•ì œ ì–¸ë˜í•‘ì„ í”¼í•˜ê³  ì•ˆì „í•œ ì²´í¬
        guard let strongSelf = self else { 
          print("âŒ [RealTime] self is nil in callback")
          return 
        }
        
        // ë…¹ìŒ ìƒíƒœ ì²´í¬ ì¶”ê°€
        guard strongSelf.isRecording else {
          print("âš ï¸ [RealTime] ë…¹ìŒ ì¤‘ì´ ì•„ë‹Œë° ì½œë°± ìˆ˜ì‹ ë¨")
          return
        }
        
        // ì§ì ‘ ì²˜ë¦¬ (ë©”ì¸ ìŠ¤ë ˆë“œ ë””ìŠ¤íŒ¨ì¹˜ ì œê±°)
        strongSelf.processAudioBuffer(buffer: buffer)
      }
      
      print("ğŸ§ [RealTime] ì˜¤ë””ì˜¤ íƒ­ ì„¤ì¹˜ ì™„ë£Œ - ë²„í¼ í¬ê¸°: \(bufferSize)")
      print("ğŸ§ [RealTime] ì½œë°±ì´ ì˜¤ë””ì˜¤ ìŠ¤ë ˆë“œì—ì„œ ì§ì ‘ ì‹¤í–‰ë¨")
      
      // CRITICAL FIX: ì˜¤ë””ì˜¤ ì—”ì§„ ì¤€ë¹„ ë° ì‹œì‘
      print("ğŸš€ [RealTime] ì˜¤ë””ì˜¤ ì—”ì§„ ì¤€ë¹„ ì¤‘...")
      
      // ì—”ì§„ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
      if engine.isRunning {
        print("âš ï¸ [RealTime] ì—”ì§„ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ - ì¬ì‹œì‘")
        engine.stop()
        engine.reset()
      }
      
      // ì—”ì§„ ì¤€ë¹„
      engine.prepare()
      print("ğŸ”§ [RealTime] ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ")
      
      // ì—”ì§„ ì‹œì‘ (ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰)
      do {
        print("ğŸš€ [RealTime] ì˜¤ë””ì˜¤ ì—”ì§„ ì‹œì‘ ì‹œë„...")
        try engine.start()
        
        // ì—”ì§„ ì‹œì‘ í›„ ìƒíƒœ í™•ì¸
        let engineRunning = engine.isRunning
        print("ğŸ” [RealTime] ì—”ì§„ ì‹œì‘ í›„ ìƒíƒœ: isRunning=\(engineRunning)")
        
        if engineRunning {
          print("âœ… [RealTime] ì—”ì§„ ì‹œì‘ ì„±ê³µ!")
          print("ğŸ¤ [RealTime] ì…ë ¥ ë²„ìŠ¤ ìˆ˜: \(input.numberOfInputs), ì¶œë ¥ ë²„ìŠ¤ ìˆ˜: \(input.numberOfOutputs)")
          
          // ì—”ì§„ ì‹œì‘ í›„ ì…ë ¥ í¬ë§· ì¬í™•ì¸
          let runningInputFormat = input.outputFormat(forBus: 0)
          print("ğŸ¤ [RealTime] ì‹¤í–‰ ì¤‘ ì…ë ¥ í¬ë§·: \(runningInputFormat)")
          print("ğŸ“Š [RealTime] ì‹¤í–‰ ì¤‘ ì±„ë„ ìˆ˜: \(runningInputFormat.channelCount)")
          
          // ì…ë ¥ ë…¸ë“œê°€ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
          if runningInputFormat.channelCount > 0 {
            print("âœ… [RealTime] ë§ˆì´í¬ ì…ë ¥ í™œì„±í™” í™•ì¸!")
          } else {
            print("âš ï¸ [RealTime] ê²½ê³ : ì—”ì§„ì€ ì‹¤í–‰ ì¤‘ì´ì§€ë§Œ ì…ë ¥ ì±„ë„ì´ 0ê°œ")
          }
        } else {
          print("âŒ [RealTime] ì—”ì§„ ì‹œì‘ ì‹¤íŒ¨")
          result(FlutterError(code: "ENGINE_START_FAILED", message: "Audio engine failed to start", details: nil))
          return
        }
      } catch {
        print("âŒ [RealTime] ì—”ì§„ ì‹œì‘ ì˜ˆì™¸: \(error)")
        result(FlutterError(code: "ENGINE_START_ERROR", message: error.localizedDescription, details: nil))
        return
      }
      
      // ë…¹ìŒ ìƒíƒœ ì„¤ì •
      isRecording = true
      
      print("âœ… [RealTime] ì‹¤ì‹œê°„ ë…¹ìŒ ì‹œì‘ ì„±ê³µ!")
      print("ğŸ“Š [RealTime] ë…¹ìŒ ìƒíƒœ: \(isRecording)")
      
      // ì‹¤ì œ ì˜¤ë””ì˜¤ ìº¡ì²˜ í™•ì¸ (ë””ë²„ê·¸ìš©)
      DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
        guard let self = self else { return }
        if self.recordedSamples.isEmpty {
          print("âš ï¸ [RealTime] 0.5ì´ˆ í›„: ì•„ì§ ì˜¤ë””ì˜¤ ìƒ˜í”Œì´ ìº¡ì²˜ë˜ì§€ ì•ŠìŒ")
          print("ğŸ” [RealTime] ì—”ì§„ ìƒíƒœ: \(self.audioEngine?.isRunning ?? false)")
          print("ğŸ¤ [RealTime] íƒ­ì´ ì„¤ì¹˜ë˜ì—ˆì§€ë§Œ ì½œë°±ì´ í˜¸ì¶œë˜ì§€ ì•ŠìŒ - ë§ˆì´í¬ í™•ì¸ í•„ìš”")
        } else {
          print("âœ… [RealTime] ì˜¤ë””ì˜¤ ìº¡ì²˜ ì¤‘: \(self.recordedSamples.count) ìƒ˜í”Œ")
        }
      }
      
      result(true)
    }
  }
  
  private func processAudioBuffer(buffer: AVAudioPCMBuffer) {
    // ë…¹ìŒ ìƒíƒœ ì¬í™•ì¸
    guard isRecording else {
      print("âš ï¸ [RealTime] processAudioBuffer í˜¸ì¶œë¨ - ë…¹ìŒ ì¤‘ì´ ì•„ë‹˜")
      return
    }
    
    let frameLength = Int(buffer.frameLength)
    let channelCount = Int(buffer.format.channelCount)
    let sampleRate = buffer.format.sampleRate
    
    // ì½œë°± í˜¸ì¶œ í™•ì¸ ë¡œê·¸ (ì²« ë²ˆì§¸ í˜¸ì¶œê³¼ 100ë²ˆì§¸ë§ˆë‹¤)
    if recordedSamples.isEmpty || recordedSamples.count % 48000 == 0 {
      print("ğŸ”„ [RealTime] *** TAP CALLBACK WORKING *** - \(frameLength) í”„ë ˆì„, \(channelCount) ì±„ë„, \(sampleRate)Hz")
      print("ğŸ¯ [RealTime] ì´ ìˆ˜ì‹ ëœ ìƒ˜í”Œ: \(recordedSamples.count)")
    }
    
    // ë²„í¼ ìœ íš¨ì„± ê²€ì‚¬
    guard frameLength > 0 && frameLength < 100000 else {
      print("âŒ [RealTime] ë¹„ì •ìƒì ì¸ í”„ë ˆì„ ê¸¸ì´: \(frameLength)")
      return
    }
    
    guard let channelData = buffer.floatChannelData else {
      print("âŒ [RealTime] ì˜¤ë””ì˜¤ ì±„ë„ ë°ì´í„°ê°€ nil")
      return
    }
    
    // ì‹¤ì œ ë°ì´í„° ìƒ˜í”Œ ê²€ì‚¬
    let firstChannelData = channelData[0]
    var hasNonZeroSample = false
    var maxSample: Float = 0.0
    
    for i in 0..<min(frameLength, 10) {  // ì²« 10ê°œ ìƒ˜í”Œë§Œ ì²´í¬
      let sample = firstChannelData[i]
      if abs(sample) > 0.0001 {
        hasNonZeroSample = true
        maxSample = max(maxSample, abs(sample))
      }
    }
    
    print("ğŸ” [RealTime] ë²„í¼ ë°ì´í„° ê²€ì‚¬ - ë¹„ì˜ ìƒ˜í”Œ: \(hasNonZeroSample), ìµœëŒ€ ê°’: \(maxSample)")
    
    // ì‹¤ì œ ë§ˆì´í¬ ì…ë ¥ë§Œ ë…¹ìŒ (í…ŒìŠ¤íŠ¸ ì‹ í˜¸ ì ˆëŒ€ ìƒì„±í•˜ì§€ ì•ŠìŒ)
    if !hasNonZeroSample {
      print("ğŸ¤ [RealTime] ë¬´ìŒ êµ¬ê°„ - ì‹¤ì œ ë§ˆì´í¬ ì…ë ¥ ëŒ€ê¸° ì¤‘")
      // ë¬´ìŒì´ì–´ë„ ê³„ì† ë…¹ìŒ (ì‹¤ì œ ë¬´ìŒë„ ë…¹ìŒì˜ ì¼ë¶€)
    }
    
    // ë””ë²„ê·¸: ë²„í¼ ìˆ˜ì‹  í™•ì¸
    if recordedSamples.count % 10000 == 0 {
      print("ğŸ¤ [RealTime] ë²„í¼ ìˆ˜ì‹ : \(frameLength) í”„ë ˆì„, \(channelCount) ì±„ë„, ì´ ìƒ˜í”Œ: \(recordedSamples.count)")
    }
    
    // ë©€í‹°ì±„ë„ ì˜¤ë””ì˜¤ë¥¼ ëª¨ë…¸ë¡œ ë‹¤ìš´ë¯¹ìŠ¤ - CRITICAL FIX
    let oldSampleCount = recordedSamples.count
    
    if channelCount == 1 {
      // ëª¨ë…¸ ì˜¤ë””ì˜¤: ì§ì ‘ ì‚¬ìš©
      let monoChannelData = channelData[0] // UnsafeMutablePointer<Float>
      print("ğŸ“Š [RealTime] ëª¨ë…¸ ì±„ë„ ë°ì´í„° ì¶”ê°€ ì¤‘... \(frameLength) ìƒ˜í”Œ")
      
      for i in 0..<frameLength {
        recordedSamples.append(monoChannelData[i])
      }
      
      print("âœ… [RealTime] ëª¨ë…¸ ìƒ˜í”Œ ì¶”ê°€ ì™„ë£Œ: \(oldSampleCount) -> \(recordedSamples.count)")
      
    } else if channelCount >= 2 {
      // ìŠ¤í…Œë ˆì˜¤/ë©€í‹°ì±„ë„ ì˜¤ë””ì˜¤: í‰ê· ìœ¼ë¡œ ë‹¤ìš´ë¯¹ìŠ¤
      let leftChannel = channelData[0] // UnsafeMutablePointer<Float>
      
      print("ğŸ“Š [RealTime] ìŠ¤í…Œë ˆì˜¤ ì±„ë„ ë°ì´í„° ë‹¤ìš´ë¯¹ìŠ¤ ì¤‘... \(frameLength) ìƒ˜í”Œ")
      
      // ì˜¤ë¥¸ìª½ ì±„ë„ì´ ìˆìœ¼ë©´ í‰ê· , ì—†ìœ¼ë©´ ì™¼ìª½ë§Œ ì‚¬ìš©
      if channelCount >= 2 {
        let rightChannel = channelData[1] // UnsafeMutablePointer<Float>
        for i in 0..<frameLength {
          let monoSample = (leftChannel[i] + rightChannel[i]) * 0.5  // L+R í‰ê· 
          recordedSamples.append(monoSample)
        }
        print("âœ… [RealTime] ìŠ¤í…Œë ˆì˜¤ L+R í‰ê·  ìƒ˜í”Œ ì¶”ê°€: \(oldSampleCount) -> \(recordedSamples.count)")
      } else {
        for i in 0..<frameLength {
          recordedSamples.append(leftChannel[i])
        }
        print("âœ… [RealTime] ì¢Œì¸¡ ì±„ë„ë§Œ ìƒ˜í”Œ ì¶”ê°€: \(oldSampleCount) -> \(recordedSamples.count)")
      }
    }
    
    // ìƒ˜í”Œ ì¶”ê°€ í™•ì¸
    let newSampleCount = recordedSamples.count
    if newSampleCount > oldSampleCount {
      print("ğŸ¯ [RealTime] ìƒ˜í”Œ ì¶”ê°€ ì„±ê³µ: \(oldSampleCount) -> \(newSampleCount) (+\(newSampleCount - oldSampleCount))")
    } else {
      print("âš ï¸ [RealTime] ìƒ˜í”Œì´ ì¶”ê°€ë˜ì§€ ì•ŠìŒ! ì´ì „: \(oldSampleCount), í˜„ì¬: \(newSampleCount)")
    }
    
    // RMS ë ˆë²¨ ê³„ì‚° (ìµœê·¼ ì¶”ê°€ëœ ìƒ˜í”Œë“¤ë¡œ)
    let recentSampleCount = min(frameLength, recordedSamples.count)
    let startIndex = max(0, recordedSamples.count - recentSampleCount)
    
    var sum: Float = 0.0
    for i in startIndex..<recordedSamples.count {
      sum += recordedSamples[i] * recordedSamples[i]
    }
    let rms = sqrt(sum / Float(recentSampleCount))
    let dbLevel = 20 * log10(max(rms, 0.000001))  // dB ë³€í™˜
    
    // CRITICAL FIX: dispose ìƒíƒœì™€ í•¨ê»˜ ì‹¤ì‹œê°„ ë ˆë²¨ ì „ì†¡
    DispatchQueue.main.async { [weak self] in
      guard let self = self, !self.isDisposed else { return }
      
      // ë„ˆë¬´ ìì£¼ í˜¸ì¶œë˜ì§€ ì•Šë„ë¡ ìƒ˜í”Œë§
      if self.recordedSamples.count % 4410 == 0 { // 0.1ì´ˆë§ˆë‹¤
        print("ğŸ“Š [RealTime] ë ˆë²¨: \(String(format: "%.1f", dbLevel))dB, ìƒ˜í”Œ: \(self.recordedSamples.count)")
        
        // channelê³¼ disposed ìƒíƒœ ì²´í¬
        if let channel = self.channel, !self.isDisposed {
          channel.invokeMethod("onAudioLevel", arguments: [
            "level": Double(dbLevel),
            "rms": Double(rms),
            "samples": self.recordedSamples.count
          ])
        }
      }
    }
  }
  
  func stopRecording(result: @escaping FlutterResult) {
    print("ğŸ›‘ [RealTime] ë…¹ìŒ ì¤‘ì§€ ìš”ì²­")
    
    guard let engine = audioEngine, let input = inputNode else {
      result(false)
      return
    }
    
    if !isRecording {
      print("âš ï¸ [RealTime] ë…¹ìŒ ì¤‘ì´ ì•„ë‹˜")
      result(true)
      return
    }
    
    // ì•ˆì „í•˜ê²Œ íƒ­ ì œê±°
    input.removeTap(onBus: 0)
    print("âœ… [RealTime] ì…ë ¥ íƒ­ ì œê±° ì™„ë£Œ")
    
    // ì—”ì§„ ì¤‘ì§€ (ì•ˆì „í•˜ê²Œ)
    if engine.isRunning {
      engine.stop()
      print("âœ… [RealTime] ì˜¤ë””ì˜¤ ì—”ì§„ ì¤‘ì§€ ì™„ë£Œ")
    }
    
    isRecording = false
    
    let duration = recordingStartTime?.timeIntervalSinceNow ?? 0
    print("âœ… [RealTime] ë…¹ìŒ ì¤‘ì§€ ì™„ë£Œ")
    print("ğŸ“Š [RealTime] ì´ ìƒ˜í”Œ: \(recordedSamples.count), ì‹œê°„: \(String(format: "%.1f", abs(duration)))ì´ˆ")
    
    // ì—”ì§„ì€ ìœ ì§€í•˜ê³  ìƒíƒœë§Œ ë¦¬ì…‹
    // ë‹¤ìŒ ë…¹ìŒì„ ìœ„í•´ ì—”ì§„ ì¬ì‚¬ìš©
    
    result(true)
  }
  
  func getRecordedAudio(result: @escaping FlutterResult) {
    print("ğŸ“ [RealTime] ë…¹ìŒ ë°ì´í„° ìš”ì²­")
    print("ğŸ“Š [RealTime] ì €ì¥ëœ ìƒ˜í”Œ ìˆ˜: \(recordedSamples.count)")
    print("ğŸ” [RealTime] ë…¹ìŒ ìƒíƒœ: isRecording=\(isRecording)")
    print("ğŸ” [RealTime] ì˜¤ë””ì˜¤ ì—”ì§„ ìƒíƒœ: \(audioEngine?.isRunning ?? false)")
    
    // ì¶”ê°€ ì§„ë‹¨ ì •ë³´
    if let _ = audioEngine, let input = inputNode {
      let inputFormat = input.outputFormat(forBus: 0)
      print("ğŸ¤ [RealTime] ì…ë ¥ í¬ë§·: \(inputFormat.sampleRate)Hz, \(inputFormat.channelCount)ì±„ë„")
      print("ğŸ”§ [RealTime] ì—”ì§„ ì—°ê²° ìƒíƒœ: ì…ë ¥ë…¸ë“œ ì—°ê²°ë¨")
    }
    
    if recordedSamples.isEmpty {
      print("âŒ [RealTime] ë…¹ìŒëœ ë°ì´í„° ì—†ìŒ - ìƒì„¸ ì§„ë‹¨ ì‹œì‘")
      
      // ìƒì„¸ ì—ëŸ¬ ì›ì¸ ë¶„ì„
      var errorMessage = "No audio samples recorded"
      var errorCode = "NO_SAMPLES"
      var diagnosticInfo: [String: Any] = [:]
      
      if audioEngine == nil {
        errorMessage = "Audio engine not initialized"
        errorCode = "ENGINE_NOT_READY"
        diagnosticInfo["issue"] = "audioEngine is nil"
      } else if let engine = audioEngine, !engine.isRunning {
        errorMessage = "Audio engine not running during recording"
        errorCode = "ENGINE_NOT_RUNNING"
        diagnosticInfo["issue"] = "engine stopped"
      } else if inputNode == nil {
        errorMessage = "Audio input node not available"
        errorCode = "INPUT_NODE_MISSING"
        diagnosticInfo["issue"] = "inputNode is nil"
      } else {
        errorMessage = "Tap callback never called - check microphone permissions and hardware"
        errorCode = "TAP_CALLBACK_FAILED"
        diagnosticInfo["issue"] = "installTap failed or callback not triggered"
        
        // ë§ˆì´í¬ ê¶Œí•œ ì¬í™•ì¸
        let micPermission = AVCaptureDevice.authorizationStatus(for: .audio)
        diagnosticInfo["micPermission"] = micPermission.rawValue
        print("ğŸ” [RealTime] ë§ˆì´í¬ ê¶Œí•œ ì¬í™•ì¸: \(micPermission.rawValue)")
      }
      
      diagnosticInfo["samplesCount"] = recordedSamples.count
      diagnosticInfo["isRecording"] = isRecording
      diagnosticInfo["engineRunning"] = audioEngine?.isRunning ?? false
      diagnosticInfo["inputNodeAvailable"] = inputNode != nil
      
      print("ğŸ” [RealTime] ìµœì¢… ì§„ë‹¨: \(errorMessage)")
      print("ğŸ“‹ [RealTime] ì§„ë‹¨ ì •ë³´: \(diagnosticInfo)")
      
      result(FlutterError(code: errorCode, message: errorMessage, details: diagnosticInfo))
    } else {
      print("âœ… [RealTime] ì‹¤ì œ ë…¹ìŒ ë°ì´í„° ë°˜í™˜: \(recordedSamples.count) ìƒ˜í”Œ")
      result(recordedSamples)
    }
  }
  
  func getRealtimeAudioBuffer(result: @escaping FlutterResult) {
    // ì‹¤ì‹œê°„ í”¼ì¹˜ ë¶„ì„ì„ ìœ„í•œ ìµœê·¼ ë²„í¼ë§Œ ë°˜í™˜ (2048 ìƒ˜í”Œ = ~0.04ì´ˆ)
    let bufferSize = 2048
    
    if recordedSamples.count < bufferSize {
      // ì•„ì§ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŒ
      result([])
      return
    }
    
    // ìµœê·¼ ë²„í¼ë§Œ ì¶”ì¶œ
    let startIdx = recordedSamples.count - bufferSize
    let recentBuffer = Array(recordedSamples[startIdx..<recordedSamples.count])
    
    print("ğŸ¤ [RealTime] ì‹¤ì‹œê°„ ë²„í¼ ë°˜í™˜: \(recentBuffer.count) ìƒ˜í”Œ")
    result(recentBuffer)
  }
  
  func loadAudioFile(path: String, result: @escaping FlutterResult) {
    print("ğŸµ [RealTime] ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë”©: \(path)")
    
    let fileURL = URL(fileURLWithPath: path)
    
    do {
      // AVAudioFileë¡œ ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°
      let audioFile = try AVAudioFile(forReading: fileURL)
      let format = audioFile.processingFormat
      let frameCount = UInt32(audioFile.length)
      
      print("ğŸ“Š [RealTime] ì˜¤ë””ì˜¤ í¬ë§·: \(format.sampleRate)Hz, \(format.channelCount)ì±„ë„, \(frameCount)í”„ë ˆì„")
      
      // ì˜¤ë””ì˜¤ ë²„í¼ ìƒì„± (ì•ˆì „í•œ í¬ê¸° ì²´í¬)
      guard frameCount > 0 && frameCount < 100_000_000 else {  // í•©ë¦¬ì ì¸ ìƒí•œì„  ì„¤ì •
        result(FlutterError(code: "INVALID_FRAME_COUNT", message: "Invalid frame count: \(frameCount)", details: nil))
        return
      }
      
      guard let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameCount) else {
        result(FlutterError(code: "BUFFER_ERROR", message: "Failed to create audio buffer", details: nil))
        return
      }
      
      // íŒŒì¼ì—ì„œ ë²„í¼ë¡œ ì½ê¸°
      try audioFile.read(into: buffer)
      buffer.frameLength = frameCount
      
      // Float ë°°ì—´ë¡œ ë³€í™˜ (ëª¨ë…¸ë¡œ ë‹¤ìš´ë¯¹ìŠ¤)
      var audioSamples: [Float] = []
      let channelCount = Int(format.channelCount)
      
      if channelCount == 1 {
        // ëª¨ë…¸ ì˜¤ë””ì˜¤
        guard let channelData = buffer.floatChannelData?[0] else {
          result(FlutterError(code: "CHANNEL_ERROR", message: "Failed to get channel data", details: nil))
          return
        }
        for i in 0..<Int(frameCount) {
          audioSamples.append(channelData[i])
        }
      } else {
        // ìŠ¤í…Œë ˆì˜¤/ë©€í‹°ì±„ë„ â†’ ëª¨ë…¸ ë‹¤ìš´ë¯¹ìŠ¤
        guard let leftChannel = buffer.floatChannelData?[0],
              let rightChannel = buffer.floatChannelData?[1] else {
          result(FlutterError(code: "CHANNEL_ERROR", message: "Failed to get channel data", details: nil))
          return
        }
        
        for i in 0..<Int(frameCount) {
          let monoSample = (leftChannel[i] + rightChannel[i]) * 0.5
          audioSamples.append(monoSample)
        }
      }
      
      print("âœ… [RealTime] ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: \(audioSamples.count) ìƒ˜í”Œ")
      
      // MP3 íŒŒì¼ì˜ ìƒ˜í”Œì„ recordedSamplesì— ì €ì¥í•˜ì—¬ ì¬ìƒ ê°€ëŠ¥í•˜ê²Œ í•¨
      self.recordedSamples = audioSamples
      print("ğŸ“Š [RealTime] MP3 ë°ì´í„°ë¥¼ ì¬ìƒ ë²„í¼ì— ì €ì¥ ì™„ë£Œ")
      
      // Flutterë¡œ ë°˜í™˜ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ)
      result([
        "samples": audioSamples,
        "sampleRate": format.sampleRate,
        "duration": Double(frameCount) / format.sampleRate
      ])
      
    } catch {
      print("âŒ [RealTime] ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: \(error)")
      result(FlutterError(code: "LOAD_ERROR", message: error.localizedDescription, details: nil))
    }
  }
  
  func playRecording(result: @escaping FlutterResult) {
    print("ğŸ”Š [RealTime] ì‹¤ì œ ì˜¤ë””ì˜¤ ì¬ìƒ ìš”ì²­")
    
    // MP3 íŒŒì¼ ë¡œë“œë¡œ ì¸í•œ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
    let samplesToPlay = !recordedSamples.isEmpty ? recordedSamples : []
    
    guard !samplesToPlay.isEmpty, !isDisposed else {
      print("âŒ [RealTime] ì¬ìƒí•  ì˜¤ë””ì˜¤ ë°ì´í„° ì—†ìŒ ë˜ëŠ” disposed")
      result(false)
      return
    }
    
    // CRITICAL FIX: ê¸°ì¡´ í”Œë ˆì´ì–´ì™€ íƒ€ì´ë¨¸ë¥¼ ì•ˆì „í•˜ê²Œ ì •ë¦¬
    cleanupPlayback()
    
    // ë…¹ìŒëœ Float ë°°ì—´ì„ ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ë¡œ ë³€í™˜
    do {
      let tempURL = createTempAudioFile(samples: samplesToPlay)
      tempFileURL = tempURL  // ì„ì‹œ íŒŒì¼ ì¶”ì 
      print("ğŸ“ [RealTime] ì„ì‹œ íŒŒì¼ ìƒì„±: \(tempURL.path)")
      
      // íŒŒì¼ ì¡´ì¬ í™•ì¸
      guard FileManager.default.fileExists(atPath: tempURL.path) else {
        print("âŒ [RealTime] ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
        result(false)
        return
      }
      
      // AVAudioSessionì€ iOS ì „ìš©ì´ë¯€ë¡œ macOSì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
      // macOSëŠ” AVAudioEngineì´ ìë™ìœ¼ë¡œ ì˜¤ë””ì˜¤ ì„¸ì…˜ì„ ê´€ë¦¬í•¨
      
      // AVAudioPlayerë¡œ ì¬ìƒ
      audioPlayer = try AVAudioPlayer(contentsOf: tempURL)
      audioPlayer?.delegate = self
      
      guard audioPlayer?.prepareToPlay() == true else {
        print("âŒ [RealTime] ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ ì¤€ë¹„ ì‹¤íŒ¨")
        result(false)
        return
      }
      
      if audioPlayer?.play() == true {
        print("âœ… [RealTime] ì‹¤ì œ ì˜¤ë””ì˜¤ ì¬ìƒ ì‹œì‘ - ê¸¸ì´: \(String(format: "%.1f", audioPlayer?.duration ?? 0))ì´ˆ")
        
        // CRITICAL FIX: ì¬ìƒ ì™„ë£Œë¥¼ ì•ˆì „í•˜ê²Œ ëª¨ë‹ˆí„°ë§
        playbackTimer = Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { [weak self] timer in
          guard let self = self, !self.isDisposed else {
            timer.invalidate()
            return
          }
          
          guard let player = self.audioPlayer else {
            timer.invalidate()
            return
          }
          
          if !player.isPlaying {
            timer.invalidate()
            print("â¸ï¸ [RealTime] ì¬ìƒ ì™„ë£Œ")
            
            // CRITICAL FIX: ì„ì‹œ íŒŒì¼ ì•ˆì „ ì‚­ì œ (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ)
            if let tempURL = self.tempFileURL {
              DispatchQueue.global(qos: .background).asyncAfter(deadline: .now() + 1.0) {
                try? FileManager.default.removeItem(at: tempURL)
              }
              self.tempFileURL = nil
            }
          }
        }
        
        result(true)
      } else {
        print("âŒ [RealTime] ì˜¤ë””ì˜¤ ì¬ìƒ ì‹œì‘ ì‹¤íŒ¨")
        cleanupPlayback()  // ì‹¤íŒ¨ ì‹œ ì •ë¦¬
        result(false)
      }
      
    } catch {
      print("âŒ [RealTime] ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨: \(error)")
      cleanupPlayback()  // ì˜ˆì™¸ ë°œìƒ ì‹œ ì •ë¦¬
      result(false)
    }
  }
  
  // CRITICAL FIX: ì¬ìƒ ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ì•ˆì „ ì •ë¦¬
  func cleanupPlayback() {
    // íƒ€ì´ë¨¸ ë¨¼ì € ì •ë¦¬
    playbackTimer?.invalidate()
    playbackTimer = nil
    
    // í”Œë ˆì´ì–´ delegate í•´ì œ í›„ ì •ì§€ (ì•ˆì „í•œ ìˆœì„œë¡œ)
    if let player = audioPlayer {
      // 1. delegateë¥¼ ë¨¼ì € í•´ì œ (ì¶”ê°€ ì½œë°± ë°©ì§€)
      player.delegate = nil
      // 2. ì¬ìƒ ì¤‘ì§€
      if player.isPlaying {
        player.stop()
      }
      // 3. ì°¸ì¡° í•´ì œ
      audioPlayer = nil
    }
    
    // ì„ì‹œ íŒŒì¼ ì •ë¦¬ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
    if let tempURL = tempFileURL {
      DispatchQueue.global(qos: .background).async {
        try? FileManager.default.removeItem(at: tempURL)
      }
      tempFileURL = nil
    }
  }
  
  private func createTempAudioFile(samples: [Float]) -> URL {
    let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
    let tempURL = documentsPath.appendingPathComponent("temp_playback_\(Int(Date().timeIntervalSince1970)).wav")
    
    // ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚­ì œ
    try? FileManager.default.removeItem(at: tempURL)
    
    // ì‹¤ì œ ë…¹ìŒëœ ìƒ˜í”Œë ˆì´íŠ¸ì™€ ë™ì¼í•˜ê²Œ ì„¤ì • (48kHz)
    let sampleRate: Double = 48000.0
    
    guard let format = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: sampleRate, channels: 1, interleaved: false) else {
      print("âŒ [RealTime] ì˜¤ë””ì˜¤ í¬ë§· ìƒì„± ì‹¤íŒ¨")
      return tempURL
    }
    
    do {
      // ì•ˆì „í•œ íŒŒì¼ ìƒì„±
      let audioFile = try AVAudioFile(forWriting: tempURL, settings: format.settings)
      
      let frameCapacity = AVAudioFrameCount(samples.count)
      guard let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameCapacity) else {
        print("âŒ [RealTime] ë²„í¼ ìƒì„± ì‹¤íŒ¨")
        return tempURL
      }
      
      // Float ìƒ˜í”Œì„ ë²„í¼ì— ì•ˆì „í•˜ê²Œ ë³µì‚¬
      guard let channelData = buffer.floatChannelData?[0] else {
        print("âŒ [RealTime] ì±„ë„ ë°ì´í„° ì ‘ê·¼ ì‹¤íŒ¨")
        return tempURL
      }
      
      let sampleCount = min(samples.count, Int(frameCapacity))
      for i in 0..<sampleCount {
        channelData[i] = samples[i]
      }
      buffer.frameLength = AVAudioFrameCount(sampleCount)
      
      try audioFile.write(from: buffer)
      print("ğŸ“ [RealTime] ì„ì‹œ ì¬ìƒ íŒŒì¼ ìƒì„±: \(sampleCount) ìƒ˜í”Œ -> \(tempURL.lastPathComponent)")
      
      // íŒŒì¼ í¬ê¸° í™•ì¸
      if let fileSize = try? FileManager.default.attributesOfItem(atPath: tempURL.path)[.size] as? Int {
        print("ğŸ“Š [RealTime] ìƒì„±ëœ íŒŒì¼ í¬ê¸°: \(fileSize) bytes")
      }
      
    } catch {
      print("âŒ [RealTime] ì„ì‹œ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: \(error)")
    }
    
    return tempURL
  }
  
  // MARK: - AVAudioPlayerDelegate
  // CRITICAL FIX: delegate ë©”ì†Œë“œì—ì„œ disposed ìƒíƒœ í™•ì¸
  func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
    guard !isDisposed else { return }  // disposed ìƒíƒœë©´ ë¬´ì‹œ
    
    print("ğŸ”Š [RealTime] ì¬ìƒ ì™„ë£Œë¨: successfully=\(flag)")
    
    // íƒ€ì´ë¨¸ ì •ë¦¬
    playbackTimer?.invalidate()
    playbackTimer = nil
    
    // í”Œë ˆì´ì–´ ì •ë¦¬ (ì¤‘ìš”: delegateë¥¼ ë¨¼ì € nilë¡œ ì„¤ì •)
    if audioPlayer === player {
      player.delegate = nil
      audioPlayer = nil
    }
    
    // ì„ì‹œ íŒŒì¼ ì•ˆì „ ì‚­ì œ
    if let tempURL = tempFileURL {
      DispatchQueue.global(qos: .background).asyncAfter(deadline: .now() + 0.5) {
        try? FileManager.default.removeItem(at: tempURL)
      }
      tempFileURL = nil
    }
  }
  
  func audioPlayerDecodeErrorDidOccur(_ player: AVAudioPlayer, error: Error?) {
    guard !isDisposed else { return }  // disposed ìƒíƒœë©´ ë¬´ì‹œ
    
    if let error = error {
      print("âŒ [RealTime] ì¬ìƒ ë””ì½”ë“œ ì˜¤ë¥˜: \(error)")
    }
    
    // ì˜¤ë¥˜ ì‹œ ì •ë¦¬
    cleanupPlayback()
  }
}

// On-device CREPE (CoreML) runner for macOS
@available(macOS 11.0, *)
class MacOnDeviceCrepeRunner {
  static let shared = MacOnDeviceCrepeRunner()
  private var model: MLModel?
  private var inputName: String?
  private init() { _ = loadModel() }

  func loadModel() -> Bool {
    if model != nil { return true }
    let bundle = Bundle.main
    let url = (
      bundle.url(forResource: "CREPE", withExtension: "mlmodelc", subdirectory: "Models") ??
      bundle.url(forResource: "CREPE", withExtension: "mlpackage", subdirectory: "Models") ??
      bundle.url(forResource: "CREPE", withExtension: "mlmodelc") ??
      bundle.url(forResource: "CREPE", withExtension: "mlpackage")
    )
    var murlOpt = url
    if murlOpt == nil {
      let fm = FileManager.default
      let cwd = fm.currentDirectoryPath
      let candidates = [
        cwd + "/macos/Runner/Models/CREPE.mlpackage",
        cwd + "/macos/Runner/Models/CREPE.mlmodelc",
        cwd + "/ios/Runner/Models/CREPE.mlpackage",
        cwd + "/ios/Runner/Models/CREPE.mlmodelc",
      ]
      for p in candidates { if fm.fileExists(atPath: p) { murlOpt = URL(fileURLWithPath: p); break } }
    }
    guard let murl = murlOpt else { print("[MacCREPE] Model not found in bundle or dev paths"); return false }
    do {
      model = try MLModel(contentsOf: murl)
      if let first = model?.modelDescription.inputDescriptionsByName.first { inputName = first.key }
      print("[MacCREPE] Model loaded: \(murl)")
      return true
    } catch {
      print("[MacCREPE] Load error: \(error)")
      model = nil
      return false
    }
  }

  func analyzeWindow(samples: Data, sampleRate: Double) -> (Double, Double) {
    guard let model = model, let inputName = inputName else { return (0.0, 0.0) }
    let count = samples.count / 4
    var f0: Double = 0.0
    var conf: Double = 0.0
    samples.withUnsafeBytes { (ptr: UnsafeRawBufferPointer) in
      let fp = ptr.bindMemory(to: Float.self)
      do {
        let arr = try MLMultiArray(shape: [NSNumber(value: count)], dataType: .float32)
        for i in 0..<count { arr[i] = NSNumber(value: fp[i]) }
        let provider = MacSimpleProvider(inputName: inputName, array: arr)
        let out = try model.prediction(from: provider)
        if let val = out.featureValue(for: "f0")?.doubleValue { f0 = val }
        if let v = out.featureValue(for: "confidence")?.doubleValue { conf = v }
        if f0 <= 0 {
          for k in ["frequency","frequencies","pitch","output"] {
            if let mv = out.featureValue(for: k)?.multiArrayValue {
              var vec = [Double]()
              for i in 0..<mv.count { vec.append(mv[i].doubleValue) }
              let filtered = vec.filter{ $0 > 0 }
              if !filtered.isEmpty { f0 = filtered.sorted()[filtered.count/2]; break }
            }
          }
        }
        if conf == 0 {
          for k in ["confidence_raw","voicing","conf"] {
            if let mv = out.featureValue(for: k)?.multiArrayValue {
              var mx = 0.0
              for i in 0..<mv.count { mx = max(mx, mv[i].doubleValue) }
              conf = mx; break
            }
          }
        }
      } catch {
        print("[MacCREPE] prediction error: \(error)")
      }
    }
    return (f0, min(max(conf, 0.0), 1.0))
  }
}

@available(macOS 11.0, *)
fileprivate class MacSimpleProvider: MLFeatureProvider {
  let inputName: String
  let array: MLMultiArray
  init(inputName: String, array: MLMultiArray) {
    self.inputName = inputName
    self.array = array
  }
  var featureNames: Set<String> { [inputName] }
  func featureValue(for featureName: String) -> MLFeatureValue? {
    if featureName == inputName { return MLFeatureValue(multiArray: array) }
    return nil
  }
}

class MainFlutterWindow: NSWindow {
  private var audioRecorder: RealTimeAudioRecorder?
  
  override func awakeFromNib() {
    let flutterViewController = FlutterViewController()
    let windowFrame = self.frame
    self.contentViewController = flutterViewController
    self.setFrame(windowFrame, display: true)

    RegisterGeneratedPlugins(registry: flutterViewController)
    setupAudioChannel(controller: flutterViewController)
    setupOnDeviceCrepeChannel(controller: flutterViewController)
    requestMicrophonePermission()

    super.awakeFromNib()
  }
  
  private func requestMicrophonePermission() {
    let status = AVCaptureDevice.authorizationStatus(for: .audio)
    
    print("ğŸ” [Main] í˜„ì¬ ë§ˆì´í¬ ê¶Œí•œ ìƒíƒœ: \(status.rawValue)")
    
    switch status {
    case .authorized:
      print("âœ… [Main] macOS ë§ˆì´í¬ ê¶Œí•œ í—ˆìš©ë¨")
      // ê¶Œí•œì´ ìˆëŠ” ê²½ìš° ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì¥ì¹˜ í™•ì¸
      checkAudioDevices()
    case .notDetermined:
      print("ğŸ”” [Main] macOS ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì¤‘...")
      AVCaptureDevice.requestAccess(for: .audio) { granted in
        DispatchQueue.main.async {
          print(granted ? "âœ… [Main] ë§ˆì´í¬ ê¶Œí•œ í—ˆìš©ë¨!" : "âŒ [Main] ë§ˆì´í¬ ê¶Œí•œ ê±°ë¶€ë¨")
          if granted {
            self.checkAudioDevices()
          }
        }
      }
    case .denied, .restricted:
      print("âŒ [Main] ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤.")
      print("ğŸ”§ [Main] ì‹œìŠ¤í…œ ì„¤ì • > ë³´ì•ˆ ë° ê°œì¸ì •ë³´ ë³´í˜¸ > ë§ˆì´í¬ì—ì„œ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.")
    @unknown default:
      print("âš ï¸ [Main] ì•Œ ìˆ˜ ì—†ëŠ” ê¶Œí•œ ìƒíƒœ")
    }
  }
  
  private func checkAudioDevices() {
    print("ğŸ¤ [Main] ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì…ë ¥ ì¥ì¹˜ í™•ì¸ ì¤‘...")
    
    let audioEngine = AVAudioEngine()
    let inputNode = audioEngine.inputNode
    let inputFormat = inputNode.outputFormat(forBus: 0)
    
    print("ğŸ¤ [Main] ê¸°ë³¸ ì…ë ¥ ì¥ì¹˜ í¬ë§·: \(inputFormat)")
    print("ğŸ“Š [Main] ìƒ˜í”Œë ˆì´íŠ¸: \(inputFormat.sampleRate)Hz, ì±„ë„: \(inputFormat.channelCount)")
  }
  
  private func setupAudioChannel(controller: FlutterViewController) {
    let channel = FlutterMethodChannel(
      name: "audio_capture",
      binaryMessenger: controller.engine.binaryMessenger
    )
    
    audioRecorder = RealTimeAudioRecorder(channel: channel)
    
    channel.setMethodCallHandler { [weak self] (call, result) in
      guard let recorder = self?.audioRecorder else {
        result(FlutterError(code: "NO_RECORDER", message: "Recorder not initialized", details: nil))
        return
      }
      
      switch call.method {
      case "startRecording":
        recorder.startRecording(result: result)
      case "stopRecording":
        recorder.stopRecording(result: result)
      case "getRecordedAudio":
        recorder.getRecordedAudio(result: result)
      case "getRealtimeAudioBuffer":
        recorder.getRealtimeAudioBuffer(result: result)
      case "loadAudioFile":
        if let args = call.arguments as? [String: Any],
           let path = args["path"] as? String {
          recorder.loadAudioFile(path: path, result: result)
        } else {
          result(FlutterError(code: "INVALID_ARGS", message: "Path required", details: nil))
        }
      case "playAudio":
        recorder.playRecording(result: result)
      case "pauseAudio":
        result(true)
      case "stopAudio":
        self?.audioRecorder?.cleanupPlayback()
        result(true)
      case "stopPlayback":
        // stopPlayback ë©”ì„œë“œ êµ¬í˜„ (stopAudioì™€ ë™ì¼í•œ ë™ì‘)
        self?.audioRecorder?.cleanupPlayback()
        result(true)
      case "seekAudio":
        result(true)
      case "getCurrentAudioLevel":
        // í˜„ì¬ ì˜¤ë””ì˜¤ ë ˆë²¨ ë°˜í™˜ - ì‹¤ì œ RMS ê³„ì‚°
        let level = recorder.getCurrentAudioLevel()
        result(level)
      default:
        result(FlutterMethodNotImplemented)
      }
    }
    
    print("âœ… [Main] ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
  }

  private func setupOnDeviceCrepeChannel(controller: FlutterViewController) {
    let channel = FlutterMethodChannel(
      name: "obiwan.ondevice_crepe",
      binaryMessenger: controller.engine.binaryMessenger
    )
    channel.setMethodCallHandler { (call, result) in
      switch call.method {
      case "analyzeWindow":
        guard let args = call.arguments as? [String: Any],
              let bytes = args["audio_bytes"] as? FlutterStandardTypedData,
              let sampleRate = args["sample_rate"] as? Double else {
          result(FlutterError(code: "BAD_ARGS", message: "audio_bytes/sample_rate required", details: nil))
          return
        }
        if #available(macOS 11.0, *) {
          let ok = MacOnDeviceCrepeRunner.shared.loadModel()
          if ok {
            let (f0, conf) = MacOnDeviceCrepeRunner.shared.analyzeWindow(samples: bytes.data, sampleRate: sampleRate)
            result(["f0": f0, "confidence": conf])
          } else {
            result(["f0": 0.0, "confidence": 0.0])
          }
        } else {
          result(["f0": 0.0, "confidence": 0.0])
        }
      default:
        result(FlutterMethodNotImplemented)
      }
    }
    print("âœ… [Main] On-device CREPE(CoreML) ì±„ë„ ì„¤ì • ì™„ë£Œ")
  }
}
